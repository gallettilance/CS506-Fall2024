{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet\n",
    "\n",
    "[FORM](https://forms.gle/hmgXYrwLn7ckapQN7)\n",
    "\n",
    "### Latent Semantic Analysis\n",
    "\n",
    "In this section we will fetch news articles from 3 different categories. We will perform Tfidf vectorization on the corpus of documents and use SVD to represent our corpus in the feature space of topics that we've uncovered from SVD. We will attempt to cluster the documents into 3 clusters as we vary the number of singular vectors we use to represent the corpus (i.e. as we vary the embedding space) and compare the output to the clustering created by the news article categories. Do we end up with a better clustering the more singular vectors we use? Is there an optimal embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "categories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\n",
    "news_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=4,max_df=0.8)\n",
    "\n",
    "stemmed_data = [\" \".join(SnowballStemmer(\"english\", ignore_stopwords=True).stem(word)  \n",
    "         for sent in sent_tokenize(message)\n",
    "        for word in word_tokenize(sent))\n",
    "        for message in news_data.data]\n",
    "\n",
    "dtm = vectorizer.fit_transform(stemmed_data)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "centered_dtm = dtm - np.mean(dtm, axis=0)\n",
    "\n",
    "u, s, vt = np.linalg.svd(centered_dtm)\n",
    "plt.xlim([0,50])\n",
    "plt.plot(range(1,len(s)+1),s)\n",
    "plt.show()\n",
    "\n",
    "ag = []\n",
    "max = len(u)\n",
    "for k in range(1,25):\n",
    "    vectorsk = u.dot(np.diag(s))[:,:k]\n",
    "    kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=10, random_state=0)\n",
    "    kmeans.fit_predict(np.asarray(vectorsk))\n",
    "    labelsk = kmeans.labels_\n",
    "    ag.append(metrics.v_measure_score(labelsk, news_data.target)) # closer to 1 means closer to news categories\n",
    "\n",
    "plt.plot(range(1,25),ag)\n",
    "plt.ylabel('Agreement',size=20)\n",
    "plt.xlabel('No of Prin Comps',size=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The data comes from the [Yelp Dataset](https://www.yelp.com/dataset). Each line is a review that consists of a label (0 for negative reviews and 1 for positive reviews) and a set of words.\n",
    "\n",
    "```\n",
    "1 i will never forget this single breakfast experience in mad...\n",
    "0 the search for decent chinese takeout in madison continues ...\n",
    "0 sorry but me julio fell way below the standard even for med...\n",
    "1 so this is the kind of food that will kill you so there s t...\n",
    "```\n",
    "\n",
    "In order to transform the set of words into vectors, we will rely on a method of feature engineering called word embeddings (Tfidf is one way to get these embeddings). Rather than simply indicating which words are present, word embeddings represent each word by \"embedding\" it in a low-dimensional vector space which may carry more information about the semantic meaning of the word. (for example in this space, the words \"King\" and \"Queen\" would be close).\n",
    "\n",
    "`word2vec.txt` contains the `word2vec` embeddings for about 15 thousand words. Not every word in each review is present in the provided `word2vec.txt` file. We can treat these words as being \"out of vocabulary\" and ignore them.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let x_i denote the sentence `“a hot dog is not a sandwich because it is not square”` and let a toy word2vec dictionary be as follows:\n",
    "\n",
    "```\n",
    "hot      0.1     0.2     0.3\n",
    "not      -0.1    0.2     -0.3\n",
    "sandwich 0.0     -0.2    0.4\n",
    "square   0.2     -0.1    0.5\n",
    "```\n",
    "\n",
    "we would first `trim` the sentence to only contain words in our vocabulary: `\"hot not sandwich not square”` then embed x_i into the feature space:\n",
    "\n",
    "$$ φ2(x_i)) = \\frac{1}{5} (word2vec(\\text{hot}) + 2 · word2vec(\\text{not}) + word2vec(\\text{sandwich}) + word2vec(\\text{square})) = \\left[0.02 \\hspace{2mm} 0.06 \\hspace{2mm} 0.12 \\hspace{2mm}\\right]^T $$\n",
    "\n",
    "a) Implement a function to trim out-of-vocabulary words from the reviews. Your function should return an nd array of the same dimension and dtype as the original loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "VECTOR_LEN = 300   # Length of word2vec vector\n",
    "MAX_WORD_LEN = 64  # Max word length in dict.txt and word2vec.txt\n",
    "\n",
    "def load_tsv_dataset(file):\n",
    "    \"\"\"\n",
    "    Loads raw data and returns a tuple containing the reviews and their ratings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the dataset tsv file.\n",
    "\n",
    "    Returns:\n",
    "        An np.ndarray of shape N. N is the number of data points in the tsv file.\n",
    "        Each element dataset[i] is a tuple (label, review), where the label is\n",
    "        an integer (0 or 1) and the review is a string.\n",
    "    \"\"\"\n",
    "    dataset = np.loadtxt(file, delimiter='\\t', comments=None, encoding='utf-8',\n",
    "                         dtype='l,O')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_feature_dictionary(file):\n",
    "    \"\"\"\n",
    "    Creates a map of words to vectors using the file that has the word2vec\n",
    "    embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): File path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary indexed by words, returning the corresponding word2vec\n",
    "        embedding np.ndarray.\n",
    "    \"\"\"\n",
    "    word2vec_map = dict()\n",
    "    with open(file) as f:\n",
    "        read_file = csv.reader(f, delimiter='\\t')\n",
    "        for row in read_file:\n",
    "            word, embedding = row[0], row[1:]\n",
    "            word2vec_map[word] = np.array(embedding, dtype=float)\n",
    "    return word2vec_map\n",
    "\n",
    "\n",
    "def trim_reviews(path_to_dataset, path_to_word2vec):\n",
    "    \"\"\"\n",
    "    Trims out-of-vocabulary words from the reviews using word2vec embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        path_to_dataset (str): Path to the dataset file.\n",
    "        path_to_word2vec (str): Path to the word2vec embedding file.\n",
    "\n",
    "    Returns:\n",
    "        A trimmed version of the dataset where reviews only contain words\n",
    "        available in the word2vec_map.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_tsv_dataset(path_to_dataset)\n",
    "\n",
    "    # Load word2vec dictionary\n",
    "    word2vec_map = load_feature_dictionary(path_to_word2vec)\n",
    "\n",
    "    # Process each review, keeping only words that exist in the word2vec_map\n",
    "    trimmed_dataset = []\n",
    "    for label, review in dataset:\n",
    "        words = review.split()  # Split review into words\n",
    "        trimmed_review = ' '.join([word for word in words if word in word2vec_map])\n",
    "        trimmed_dataset.append((label, trimmed_review))\n",
    "\n",
    "    return np.array(trimmed_dataset, dtype=dataset.dtype)\n",
    "\n",
    "# Usage\n",
    "trim_train = trim_reviews(\"./data/train_small.tsv\", \"./data/word2vec.txt\")\n",
    "trim_test = trim_reviews(\"./data/test_small.tsv\", \"./data/word2vec.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Implement the embedding and store it to a .tsv file where the first column is the label and the rest are the features from the embedding. Round all numbers to 6 decimal places. embedded_train_small.tsv contains the expected output of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_reviews(trimmed_dataset, word2vec_map):\n",
    "    \"\"\"\n",
    "    Embed the reviews by averaging word embeddings of in-vocabulary words.\n",
    "\n",
    "    Parameters:\n",
    "        trimmed_dataset (np.ndarray): Dataset with reviews already trimmed.\n",
    "        word2vec_map (dict): Dictionary of word2vec embeddings.\n",
    "\n",
    "    Returns:\n",
    "        A list where each entry is a tuple (label, embedding vector).\n",
    "    \"\"\"\n",
    "    embedded_dataset = []\n",
    "    \n",
    "    for label, review in trimmed_dataset:\n",
    "        words = review.split()  # Split the review into words\n",
    "        vectors = [word2vec_map[word] for word in words if word in word2vec_map]  # Get embeddings\n",
    "        \n",
    "        if vectors:  # Ensure there are embeddings to average\n",
    "            avg_embedding = np.mean(vectors, axis=0)  # Average the vectors\n",
    "        else:\n",
    "            avg_embedding = np.zeros(VECTOR_LEN)  # Use a zero vector if no embeddings exist\n",
    "        \n",
    "        embedded_dataset.append((label, avg_embedding))\n",
    "    \n",
    "    return embedded_dataset\n",
    "\n",
    "\n",
    "def save_as_tsv(dataset, filename):\n",
    "    \"\"\"\n",
    "    Save the embedded dataset to a .tsv file.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (list): List of (label, embedding vector) tuples.\n",
    "        filename (str): The file path where the dataset should be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'w+') as f:\n",
    "        for label, embedding in dataset:\n",
    "            embedding_str = '\\t'.join([f\"{x:.6f}\" for x in embedding])  # Format embedding with 6 decimal places\n",
    "            f.write(f\"{label}\\t{embedding_str}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "word2vec_map = load_feature_dictionary(\"./data/word2vec.txt\")  # Load the word2vec map\n",
    "\n",
    "# Embed the trimmed datasets\n",
    "embedded_train = embed_reviews(trim_train, word2vec_map)\n",
    "embedded_test = embed_reviews(trim_test, word2vec_map)\n",
    "\n",
    "# Save to .tsv files\n",
    "save_as_tsv(embedded_train, \"./data/output/embedded_train_small.tsv\")\n",
    "save_as_tsv(embedded_test, \"./data/output/embedded_test_small.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the form. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.337376\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "row_num = 0\n",
    "with open('./data/output/embedded_test_small.tsv') as f:\n",
    "    read_file = csv.reader(f, delimiter='\\t')\n",
    "    for row in read_file:\n",
    "        if row_num == 6:\n",
    "            print(row[12])\n",
    "            break\n",
    "        row_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
